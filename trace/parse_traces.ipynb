{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.core.debugger import set_trace\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: parsing from file. this could a minute or two...\n",
      "INFO: saving to json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRACE_FILES = glob.glob(\n",
    "    \"/data/pooya/gem5/runs/rowhammer/2024-03-11_10-51-12/traces/*.stdout\"\n",
    ")\n",
    "\n",
    "TRACE_FILE = TRACE_FILES[0]\n",
    "\n",
    "# parse each line with regex\n",
    "fields = r\"^([0-9]+).*mem_side.*MEM\\s(\\w+)\\s\\[(\\w+):(\\w+)\\]\"\n",
    "\n",
    "# all accessed_addressed\n",
    "accessed_addresses = {}\n",
    "sorted_addresses = {} # the same thing but sorted :p\n",
    "clocked_accesses = {} # list of lists\n",
    "\n",
    "first_lined = False\n",
    "init_timestamp = 0\n",
    "\n",
    "s = set()\n",
    "if \".json\" in TRACE_FILE:\n",
    "    print(\"INFO: loading from json\")\n",
    "    accessed_addresses = json.load(open(TRACE_FILE, 'r'))\n",
    "else:\n",
    "    print(\"INFO: parsing from file. this could a minute or two...\")\n",
    "    counter = 0\n",
    "    with open(TRACE_FILE, 'r') as f:\n",
    "        # parse out the first line for some key numbers\n",
    "        while True:\n",
    "            for line in f:\n",
    "                result = re.match(fields, line)\n",
    "                if result:\n",
    "                    init_timestamp = int(result[1])\n",
    "                    timestamp = int(result[1])\n",
    "                    op = result[2]\n",
    "                    start_address = int(result[3], 16)\n",
    "                    end_address = int(result[4], 16)       \n",
    "                    ms_block = (timestamp - init_timestamp) // 64000000000\n",
    "                    break\n",
    "            break\n",
    "        for line in f:\n",
    "            result = re.match(fields, line)\n",
    "            if result:\n",
    "                timestamp = int(result[1])\n",
    "                op = result[2]\n",
    "                start_address = int(result[3], 16)\n",
    "                end_address = int(result[4], 16)\n",
    "\n",
    "                ms_block = (timestamp - init_timestamp) // 64000000000\n",
    "                s.add(ms_block)\n",
    "                if ms_block not in clocked_accesses:\n",
    "                    clocked_accesses[ms_block] = []\n",
    "                clocked_accesses[ms_block].append((timestamp, start_address, end_address, op))\n",
    "\n",
    "                if ms_block not in accessed_addresses:\n",
    "                    accessed_addresses[ms_block] = {}\n",
    "                accessed_addresses[ms_block][start_address] = accessed_addresses[ms_block].get(start_address, 0) + 1\n",
    "\n",
    "    print(\"INFO: saving to json\")\n",
    "    with open('accessed_addresses.json', 'w') as f:\n",
    "        json.dump(accessed_addresses, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency:  27535.015568455972\n"
     ]
    }
   ],
   "source": [
    "# get average mem access time accross entire trace\n",
    "\n",
    "fields = r\"^([0-9]+).*(\\w\\w\\w)_side.*MEM\\s(\\w+)\\s\\[(\\w+):(\\w+)\\]\"\n",
    "counter = 0\n",
    "matches = {}\n",
    "latency_sum = 0\n",
    "\n",
    "with open(TRACE_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        result = re.match(fields, line)\n",
    "        if result:\n",
    "            timestamp = int(result[1])\n",
    "            tp = result[2]\n",
    "            op = result[3]\n",
    "            start_address = int(result[4], 16)\n",
    "            end_address = int(result[5], 16)\n",
    "            counter += 1\n",
    "            try:\n",
    "                if tp == \"mem\":\n",
    "                    matches[start_address] = timestamp\n",
    "                else:\n",
    "                    latency_sum += timestamp - matches[start_address]\n",
    "                    # remove that address from the list\n",
    "                    # matches.pop(start_address, None)\n",
    "                    del matches[start_address]\n",
    "            except:\n",
    "                pass\n",
    "            if counter > 5000000:\n",
    "                break\n",
    "\n",
    "    avg_dram_latency = latency_sum / counter\n",
    "    print(\"Average latency: \", avg_dram_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving \n"
     ]
    }
   ],
   "source": [
    "# save the accessed addresses to a json file\n",
    "sorted_accesses = {}\n",
    "for chunk in accessed_addresses.items():\n",
    "    sorted_dict = dict(sorted(chunk[1].items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_accesses[chunk[0]] = sorted_dict\n",
    "    # print the top 10 items in the sorted dict\n",
    "# save the sorted_access json to a file\n",
    "print(\"Saving \")\n",
    "with open('sorted_accesses.json', 'w') as f:\n",
    "    json.dump(sorted_accesses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a cache\n",
    "\n",
    "class Cache:\n",
    "\n",
    "    def __init__(self, size=2**15, set_assoc=8, line_size=8):\n",
    "        # cache lines are just tuples, where first elemetnt is tag, second is valid bit, and third is the data\n",
    "        # it's a list and indexed by the set index\n",
    "        # each set then contains a list of ways\n",
    "\n",
    "        # sanity checks\n",
    "        if size % line_size != 0:\n",
    "            raise ValueError(\"Cache size must be divisible by line size\")\n",
    "        if size % set_assoc != 0:\n",
    "            raise ValueError(\"Cache size must be divisible by set associativity\")\n",
    "        if line_size % 2 != 0:\n",
    "            raise ValueError(\"Line size must be divisible by 2\")\n",
    "\n",
    "        self.size = size\n",
    "        self.set_assoc = set_assoc\n",
    "        self.line_size = line_size\n",
    "        self.num_lines = size // line_size\n",
    "        self.num_sets = self.num_lines // self.set_assoc\n",
    "\n",
    "        self.bo_size = int(math.log2(self.line_size))\n",
    "        self.bo_bit_mask = (1 << self.bo_size) - 1\n",
    "\n",
    "        self.set_index_size = int(math.log2(self.num_lines // self.set_assoc))\n",
    "        self.set_index_bit_mask = (1 << self.set_index_size) - 1\n",
    "\n",
    "        self.tag_shift_size = self.set_index_size + self.bo_size \n",
    "        # no need for a tag bit mask\n",
    "        # print(self.bo_size, self.set_index_size, self.tag_size)\n",
    "\n",
    "        self.sets = [[{\"way\": x, \"tag\": 0, \"valid\": 0, \"data\": 0, \"last_access\":0} for x in range(self.set_assoc)] for _ in range(self.num_sets)]\n",
    "\n",
    "        self.init_stats()\n",
    "\n",
    "        print(\"INFO: Cache initialized\")\n",
    "        print(f\"INFO: cache params: size: {self.size} set_assoc: {self.set_assoc} line_size: {self.line_size}, num_lines: {self.num_lines}, num_sets: {self.num_sets}\")\n",
    "\n",
    "        # self.sets = [CacheSet(SET_ASSOCIATIVITY) for _ in range(NUM_LINES // SET_ASSOCIATIVITY)]\n",
    "    \n",
    "    def read(self, address, timestamp):\n",
    "        \"\"\"Access the cache at the given address. Returns data if hit, False if miss.\"\"\"\n",
    "        \n",
    "        block_bits = address & self.bo_bit_mask\n",
    "        set_index = (address >> self.bo_size) & self.set_index_bit_mask\n",
    "        tag_bits = address >> self.tag_shift_size\n",
    "\n",
    "        for i in self.sets[set_index]:\n",
    "            if i[\"tag\"] == tag_bits and i[\"valid\"] == 1:\n",
    "                # sanity check\n",
    "                if i[\"last_access\"] > timestamp:\n",
    "                    raise ValueError(\"Timestamps are not monotonically increasing\")\n",
    "                i[\"last_access\"] = timestamp\n",
    "                self.read_hits += 1\n",
    "                return i[\"data\"] # return the data\n",
    "\n",
    "        self.read_misses += 1\n",
    "        return False\n",
    "    \n",
    "    def write(self, address, data, timestamp):\n",
    "        \"\"\"Write to the cache at the given address. Returns None if the write was successful (write hit) returns the address and data of the evicted line.\"\"\"\n",
    "\n",
    "        block_bits = address & self.bo_bit_mask\n",
    "        set_index = (address >> self.bo_size) & self.set_index_bit_mask\n",
    "        tag_bits = address >> self.tag_shift_size\n",
    "\n",
    "        # the write policy should write to the first way that is invalid\n",
    "        s = self.sets[set_index]\n",
    "\n",
    "        # first check the tags, the line is already in the cache, just write to it\n",
    "        for i in self.sets[set_index]:\n",
    "            if i[\"tag\"] == tag_bits and i[\"valid\"] == 1:\n",
    "                i[\"data\"] = data\n",
    "                i[\"last_access\"] = timestamp\n",
    "                self.write_hits += 1\n",
    "                return None\n",
    "        for i in self.sets[set_index]:\n",
    "            if i[\"valid\"] == 0:\n",
    "                # print(\"found invalid line\")\n",
    "                c = i.copy()\n",
    "                i[\"valid\"] = 1\n",
    "                i[\"tag\"] = tag_bits\n",
    "                i[\"data\"] = data\n",
    "                i[\"last_access\"] = timestamp\n",
    "                self.write_misses += 1\n",
    "                return c\n",
    "        \n",
    "        # if we reach this point, every line is valid, and we need to evict one\n",
    "        self.write_misses += 1\n",
    "        self.write_evictions += 1\n",
    "        #find LRU\n",
    "        lru = math.inf\n",
    "        evicted = None\n",
    "        for i in self.sets[set_index]:\n",
    "            if i[\"last_access\"] < lru:\n",
    "                lru = i[\"last_access\"]\n",
    "                evicted = i.copy()\n",
    "        for i in self.sets[set_index]:\n",
    "            if i[\"last_access\"] == lru:\n",
    "                i[\"tag\"] = tag_bits\n",
    "                i[\"data\"] = data\n",
    "                i[\"last_access\"] = timestamp\n",
    "            \n",
    "        return evicted\n",
    "\n",
    "    def print_set(self, set_index):\n",
    "        \"\"\"Print the contents of a set\"\"\"\n",
    "        s = self.sets[set_index]\n",
    "        print(f\"Set {set_index}\")\n",
    "        print(json.dumps(s))\n",
    "\n",
    "    def dump_contents(self):\n",
    "        \"\"\"Dump contents for debugging\"\"\"\n",
    "\n",
    "        for i, s in enumerate(self.sets):\n",
    "            print(\"Set\", i)\n",
    "            for k in s:\n",
    "                print(k)\n",
    "\n",
    "    def init_stats(self):\n",
    "        print(\"INFO: init stats\")\n",
    "        self.read_hits = 0\n",
    "        self.read_misses = 0\n",
    "\n",
    "        self.write_hits = 0\n",
    "        self.write_misses = 0        \n",
    "        self.write_evictions = 0\n",
    "\n",
    "    # return a namespace of the stats\n",
    "    def get_stats(self):\n",
    "        return SimpleNamespace(**{\n",
    "            \"read_hits\": self.read_hits,\n",
    "            \"read_misses\": self.read_misses,\n",
    "            \"write_hits\": self.write_hits,\n",
    "            \"write_misses\": self.write_misses,\n",
    "            \"write_evictions\": self.write_evictions\n",
    "        })\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print(\"INFO: read hits:\", self.read_hits)\n",
    "        print(\"INFO: read misses:\", self.read_misses)\n",
    "        print(\"INFO: write hits:\", self.write_hits)\n",
    "        print(\"INFO: write misses:\", self.write_misses)\n",
    "        print(\"INFO: write evictions:\", self.write_evictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: init stats\n",
      "INFO: Cache initialized\n",
      "INFO: cache params: size: 32768 set_assoc: 8 line_size: 8, num_lines: 4096, num_sets: 512\n",
      "INFO: -------------\n",
      "INFO: test_cache STATS\n",
      "INFO: read hits: 0\n",
      "INFO: read misses: 0\n",
      "INFO: write hits: 6\n",
      "INFO: write misses: 1\n",
      "INFO: write evictions: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# code to use the cache\n",
    "test_cache = Cache()\n",
    "\n",
    "# testing the test_cache\n",
    "addr = 0x12345678\n",
    "# test_cache.read(addr, 10)\n",
    "test_cache.write(addr, 0xdeadbeef, 2) # Write miss\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "test_cache.write(addr, 0xdeadbeef, 3) # write hit\n",
    "\n",
    "# test_cache.dump_contents()\n",
    "print(\"INFO: -------------\")\n",
    "print(\"INFO: test_cache STATS\")\n",
    "test_cache.print_stats()\n",
    "\n",
    "len(clocked_accesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11592\n",
      "INFO: init stats\n",
      "INFO: Cache initialized\n",
      "INFO: cache params: size: 32768 set_assoc: 8 line_size: 8, num_lines: 4096, num_sets: 512\n",
      "INFO: read hits: 11542\n",
      "INFO: read misses: 50\n",
      "INFO: write hits: 0\n",
      "INFO: write misses: 50\n",
      "INFO: write evictions: 0\n"
     ]
    }
   ],
   "source": [
    "# test the counter cache\n",
    "\n",
    "# 32 GB  of memory.\n",
    "# 8KB rows\n",
    "# 4M lines\n",
    "\n",
    "print(len(clocked_accesses[0]))\n",
    "cache = Cache(size=2**15)\n",
    "for i in clocked_accesses[0]:\n",
    "    timestamp = i[0]\n",
    "    start_addr = i[1]\n",
    "    end_addr = i[2]\n",
    "    op = i[3]\n",
    "\n",
    "    # convert start_addr to row_address\n",
    "    addr = start_addr >> 13\n",
    "    if \"Read\" in op:\n",
    "        # print(\"READ\", op)\n",
    "        stat = cache.read(addr, timestamp)\n",
    "        if not stat:\n",
    "            cache.write(addr, 1, timestamp)\n",
    "    else:\n",
    "        # print(\"WRITE\", op)\n",
    "        cache.write(addr, 1, timestamp)\n",
    "\n",
    "cache.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need to simulate a cache.\n",
    "# we have the clocked accesses.\n",
    "# start with the second block, since the first is almost garuenteed to not the full 64ms window\n",
    "\n",
    "# how does our methodology work?\n",
    "# 1. Upon if we have a cache hit, return data to cpu and issue REFs to nearby rows, reset counter\n",
    "# 2. maintain counters per row\n",
    "# 3. perhaps not on every access do we need to cache the data. but maybe we cache it whenever a certain value on the counter is reached\n",
    "\n",
    "# 32 GB of memory space = 8M lines\n",
    "DRAM_SIZE = 32 * 2**30\n",
    "ROW_SIZE = 8 * 2**10\n",
    "NUM_ROWS = DRAM_SIZE // ROW_SIZE\n",
    "WIDTH = 8 # in bits\n",
    "\n",
    "# a row counter object. we will then have a list of these\n",
    "class RowCounters():\n",
    "    def __init__(self):\n",
    "        self._width = 8 # in bits\n",
    "        self._counters = [0 for _ in range(NUM_ROWS)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: support slicing at some point\n",
    "        return self._counters[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        # TODO: support slicing at some point\n",
    "        if index < 0 or index >= NUM_ROWS:\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        # if value > 2**WIDTH-1:\n",
    "        #     print(\"WARNING: counter spill over\")\n",
    "        self._counters[index] = value\n",
    "\n",
    "    def __contains__(self, index):\n",
    "        if index < 0 or index >= NUM_ROWS:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return NUM_ROWS\n",
    "\n",
    "    def convert_address_to_row(self, address) -> int:\n",
    "        \"\"\"Returns a row index given an address.\"\"\"\n",
    "        return address // ROW_SIZE\n",
    "    \n",
    "    def check_counters(self, threshold=2**(WIDTH-1)) -> list:\n",
    "        \"\"\"Returns a list of indeces of rows that have reached the threshold.\"\"\"\n",
    "        # check if any counters\n",
    "        ind = []\n",
    "        for i in range(NUM_ROWS):\n",
    "            if self[i] >= threshold:\n",
    "                ind.append(i)\n",
    "        return ind\n",
    "    \n",
    "    def clear_all(self):\n",
    "        for i in range(NUM_ROWS):\n",
    "            self[i] = 0\n",
    "\n",
    "    def get_sorted(self):\n",
    "        \"\"\"returns a dictionary sorted by the counter value\"\"\"\n",
    "        d = {}\n",
    "        for i in range(NUM_ROWS):\n",
    "            d[i] = self[i]\n",
    "        return dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RowCounters()\n",
    "\n",
    "for access in clocked_accesses[0]:\n",
    "    timestamp = access[0]\n",
    "    start_address = access[1]\n",
    "    end_address = access[2]\n",
    "    op = access[3]\n",
    "\n",
    "    row = c.convert_address_to_row(start_address)\n",
    "    c[row] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, we have the entire list of offending rows.\n",
    "# we need a further breakdown of the accesses to each row.\n",
    "\n",
    "accesses_per_row = {}\n",
    "row_accesses = c.get_sorted() # get the sorted list of rows\n",
    "for s in row_accesses:\n",
    "    if c[s] != 0:\n",
    "        accesses_per_row[s] = {}\n",
    "        accesses_per_row[s][\"total count\"] = c[s]\n",
    "\n",
    "# now loop through all accesses and count them\n",
    "for access in accessed_addresses[0]:\n",
    "    row = c.convert_address_to_row(access)\n",
    "    if row in accesses_per_row:\n",
    "        accesses_per_row[row][access] = accessed_addresses[0][access]\n",
    "\n",
    "# save the accesses_per_row to a json file\n",
    "with open('accesses_per_row.json', 'w') as f:\n",
    "    json.dump(accesses_per_row, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: total mem accesses in a 64ms period 11592\n",
      "INFO: total cache accesses in a 64ms period 11642\n",
      "INFO: total number of rows accessed in a 64ms period 11592\n",
      "100.0 146.168\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "INFO: Worst case extra latency 0.03\n",
      "INFO: Worst case overhead: 0.05%\n",
      "INFO: Realistic overhead: 0.03%\n"
     ]
    }
   ],
   "source": [
    "# calculate feasability\n",
    "\n",
    "stats = cache.get_stats()\n",
    "\n",
    "# total mem accesses in a 64ms period\n",
    "tot_num_mem_accesses = len(clocked_accesses[0])\n",
    "tot = stats.read_hits + stats.read_misses + stats.write_hits + stats.write_misses\n",
    "\n",
    "print(\"INFO: total mem accesses in a 64ms period\", tot_num_mem_accesses)\n",
    "print(\"INFO: total cache accesses in a 64ms period\", tot)\n",
    "\n",
    "num_row_accessed = len(accessed_addresses[0])\n",
    "print(\"INFO: total number of rows accessed in a 64ms period\", num_row_accessed)\n",
    "\n",
    "\n",
    "# baseline refreshes that must always occur\n",
    "extra_refreshes = stats.read_misses + stats.write_misses\n",
    "\n",
    "bs = ((stats.read_hits + stats.write_hits) / num_row_accessed) // 250 + extra_refreshes\n",
    "ws = (stats.read_hits + stats.write_hits) / 250 + extra_refreshes\n",
    "\n",
    "\n",
    "# worst case extra latency\n",
    "# bs_el = bs * avg_dram_latency / 1000\n",
    "ws_el = ws * avg_dram_latency / 1000 / 1000 / 1000 * 8\n",
    "ws_overhead = ws_el / 64 * 100\n",
    "\n",
    "# more realistic refreshes\n",
    "for i,v in accesses_per_row.items():\n",
    "    c = v[\"total count\"] // 250\n",
    "    print(c)\n",
    "    extra_refreshes += c\n",
    "\n",
    "real_overhead = extra_refreshes * avg_dram_latency / 1000 / 1000 / 1000 / 64 * 100 * 8\n",
    "print(\"INFO: Worst case extra latency\", round(ws_el, 2))\n",
    "print(f\"INFO: Worst case overhead: {round(ws_overhead, 2)}%\")\n",
    "print(f\"INFO: Realistic overhead: {round(real_overhead, 2)}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8508 128\n",
      "8509 128\n",
      "8510 128\n",
      "8511 128\n",
      "8512 128\n",
      "8513 128\n",
      "8514 128\n",
      "8515 128\n",
      "8516 128\n",
      "8517 128\n",
      "8518 128\n",
      "8519 128\n",
      "8520 128\n",
      "8521 128\n",
      "8522 128\n",
      "8523 128\n",
      "8524 128\n",
      "8525 128\n",
      "8526 128\n",
      "8527 128\n"
     ]
    }
   ],
   "source": [
    "# this block of code just to read some jsons, should be independent of the above code\n",
    "# TODO: remove this later\n",
    "\n",
    "j = json.load(open('lbm/accesses_per_row.json', 'r'))\n",
    "\n",
    "counter = 20\n",
    "for i in j:\n",
    "    print(i, j[i]['total count'])\n",
    "    counter -= 1\n",
    "    if counter == 0:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
